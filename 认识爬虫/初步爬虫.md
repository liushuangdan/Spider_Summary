
初步爬虫
===

---

### 爬取python官网首页

```python

import requests

url = 'https://www.python.org/'
response = requests.get(url)
with open('python.html','wb') as f:
    f.write(response.content)

```
---

### 百度首页的爬取

```python
import requests
import json
#需要有输入 ， url
url = 'http://www.baidu.com'
#函数的主体
response = requests.get(url)
#打印输出，text 就是response 内存储的数据的内容
# print(type(response.text))
# print(type(response.content))

#直接做存储
with open('baidu.html','wb') as f:
    f.write(response.content)

```
---

### 百度翻译的爬取

```python
import requests

url = "https://fanyi.baidu.com/sug"

#生成一个 form 的字典 必须是字典
form = {
    'kw':'粉'
}
#爬虫第三步

response = requests.post(url,data=form)
#json()函数就是返回给我们python对象的类
#equals res_dict = json.loads(response.text)
res_dict = response.json()
#获取我们需要的信息 爬虫中的第四步
translated = res_dict['data'][0]['v']

print('粉的翻译就是：'+ translated)
# print(response.text)
```
---

##Requests 库的进阶用法
### 人人网的初步爬取

#### 人人网登陆需要cookie时

```python
import requests

url = 'http://www.renren.com/967976797/profile'

#当需要登录比较麻烦的时候，我们可以使用已经登录过的内容的cookie值进行访问，登陆后才能访问的页面
headers = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    # 'Accept-Encoding': 'gzip, deflate, sdch',
    'Accept-Language': 'zh-CN,zh;q=0.8',
    'Cache-Control': 'max-age=0',
    'Connection': 'keep-alive',
    'Cookie': 'anonymid=jlxl0mggdo5rqv; depovince=GW; _r01_=1; jebe_key=7f2d6822-37f7-4ab7-b367-e13c8670fc12%7C17348ebab6f4bb4ad4fa1da95d483e7d%7C1536663196307%7C1%7C1536663415354; jebe_key=7f2d6822-37f7-4ab7-b367-e13c8670fc12%7C17348ebab6f4bb4ad4fa1da95d483e7d%7C1536663196307%7C1%7C1536663418560; JSESSIONID=abcicUQJGC0bZ6OexDixw; __utmt=1; __utma=151146938.2068577366.1536669084.1536669084.1536669084.1; __utmb=151146938.1.10.1536669084; __utmc=151146938; __utmz=151146938.1536669084.1.1.utmcsr=renren.com|utmccn=(referral)|utmcmd=referral|utmcct=/967976797/profile; vip=1; jebecookies=ea86b031-6370-4ee7-84eb-91e9c574ef1f|||||; ick_login=21b202fd-23b1-405c-9c9d-9013aecb58b6; _de=5F464D920D5EAF281D007C7E767C48C1; p=8526c37f72970154096ae1f748ed16027; first_login_flag=1; ln_uact=18665945450; ln_hurl=http://head.xiaonei.com/photos/0/0/men_main.gif; t=b724f429d33707415176a6ad2ca6b8427; societyguester=b724f429d33707415176a6ad2ca6b8427; id=967976797; xnsid=375cf4b7; loginfrom=syshome; wp_fold=0',
    'Host': 'www.renren.com',
    'Referer': 'http://zhibo.renren.com/top',
    'Upgrade-Insecure-Requests': '1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
}

response = requests.get(url, headers=headers)

with open('renren6.html', 'wb') as f:
    f.write(response.content)
```
**第二种方式**

```python
import requests

#因为需要保存cookie，所以我们需要requests中保存cookie的类，这个类叫做session
s = requests.session()

url = 'http://www.renren.com/PLogin.do'

form = {
    'email': '18665945450',
    'password': '802323'
}

#用s(session)替代requests,就能够保存cookie到session中
response = s.post(url,data=form)

with open('renren.html','wb') as f:
    f.write(response.content)

url_private = 'http://www.renren.com/privacyhome.do'

response = s.get(url_private)

with open('private.html', 'wb') as f:
    f.write(response.content)
```

**登陆人人网时，会出现一个登陆页面，登陆页面会触发一个Ajax，登陆页面又是一个form表单的提交页面，所以说这个时候就需要另一种方式来爬取信息。**

```python
import requests

s = requests.session()

url = 'http://www.renren.com/ajaxLogin/login?1=1&uniqueTimestamp=2018821911387'

form = {
    'email': '18665945450',
    'icode': '',
    'origURL:http': 'http://www.renren.com/home',
    'domain': 'renren.com',
    'key_id': '1',
    'captcha_type': 'web_login',
    'password': '6456327ee115979e4bd8e765ec5833711be3dd1599a11d1166a2a10aa63d4b8b',
    'rkey': 'bf2984d600797ab045171eb14aa24b9f',
    'f': 'http%3A%2F%2Fzhibo.renren.com%2Ftop',
}

response = s.post(url,data=form)
home_url = 'http://www.renren.com/Home.do'
response = s.get(home_url)
with open('home.html', 'wb') as f:
    f.write(response.content)


# print(response.text)
#{"code":true,"homeUrl":"http://www.renren.com/Home.do"}
```

---

## 图片的爬取

```python
import requests

import json

import os
url = 'https://www.toutiao.com/search_content/?offset=60&format=json&keyword=%E5%B8%85%E5%93%A5%E5%9B%BE%E7%89%87&autoload=true&count=20&cur_tab=1&from=search_tab'

response = requests.get(url)
# 主要有一个文件夹来存储图片
dictionary_image = 'download'
# 判断是否存在download文件夹
if not os.path.exists(dictionary_image):
    # 如果不存在就创建这个文件夹
    os.mkdir(dictionary_image)

# json中的loads函数是，从字符串转换成json的类型，可能是字典，也可能是列表
# 传入的信息就是 html_str
html_dict = json.loads(response.text)
data_list = html_dict['data']
# 通过html_dict， 能够以字典的方式获取到image的列表
for data in data_list:
    # 判断是否存在image_list
    if 'image_list' not in data:
        continue
    img_list = data['image_list']
    # 通过image的列表获取到每一个image
    for img in img_list:
        image_url = 'http:' + img['url']
        # 图片的文件名需要指定不一样的名字，所以我们去了url的最后的那个字符串，以/分割
        filename = dictionary_image +'/' +image_url.split('/')[-1] + '.jpg'
        # 需要下载图片，就用requests
        image_res = requests.get(image_url)
        with open(filename, 'wb') as f:
            f.write(image_res.content)
```

---

## 微博的爬取，需要信息入库。

```python
import requests
import mysqlhelper
# 1. 确定的需求是 某某某的微博 的微博内容的获取
# 2. 找到了手机端的内容，因为网页端需要登录， 太**
base_url = 'https://m.weibo.cn/api/container/getIndex?uid=xxxxxxxxxx&luicode=10000011&lfid=100103type%3D1%26q%3D%E4%BD%95%E7%82%85&type=uid&value=xxxxxxxx&containerid=107603xxxxxxxxx&page={}'

# helper = mysqlhelper.MysqlHelper()
# 下载所有的返回值
for i in range(0, 20):
    url = base_url + '&page=' + str(i)
    response = requests.get(url)
    res_dict = response.json()
    # 需要找到的内容首先是data， cards，然后对cards循环， 找到mgblog下的created_at 和 text
    cards_list = res_dict['data']['cards']
    print('download... page' + str(i))
    for card in cards_list:
        if 'mblog' not in card:
            continue
        created_at = card['mblog']['created_at']
        text = card['mblog']['text']
        print(created_at)
        print(text)
        # 我们插入的时候的insert语句
        # insert_sql  = 'insert into weibo(created_at, context) values(%s, %s)'
        # 需要用到的数据的内容
        # data = (created_at, text)
        # print(text)
        # 执行这个insert语句，将数据存储到mysql中
        # helper.execute_modify_sql(insert_sql, data)

# truncate 表明------清空表
```
---

## 信息入库需要记住以及了解的代码

```python
import pymysql

class MysqlHelper(object):
    def __init__(self):
    #数据库的用户名，密码，数据库的库名，字符编码级
        self.conn = pymysql.connect(host='127.0.0.1', port=3306, user='root', password='123456', db='py12', charset='utf8mb4')
        self.cursor = self.conn.cursor()

    def execute_modify_sql(self, insert_sql, data):
        self.cursor.execute(insert_sql, data)
        self.conn.commit()

    def __del__(self):
        self.cursor.close()
        self.conn.close()
#测试 是否可以插入信息成功
if __name__ == '__main__':
    insert_sql = 'insert into renxing (c1, c2) values(%s, %s)'
    data = (1, "不准任性'")
    helper = MysqlHelper()
    helper.execute_modify_sql(insert_sql, data)
```

----

### 有道翻译 可以做一个自己的英语字典  哈哈~~

```python
import requests
import time
import random
import hashlib

def translate(trans_str):
    # 通过分析javascript的代码可知， 我们需要找到的是：
    # salt = time + 随机数
    salt = int(time.time()*1000) + random.randint(0, 10)
    salt = str(salt)
    sign = "fanyideskweb" + trans_str + salt + "6x(ZHw]mwzX#u0V7@yfwK"

    # 获取sign的md5的值
    md5_value = hashlib.md5()
    md5_value.update(sign.encode('utf-8'))
    sign = md5_value.hexdigest()

    # sign

    headers ={
        'Cookie': 'OUTFOX_SEARCH_USER_ID_NCOO=1891973180.1976018; _ga=GA1.2.1933331030.1534818606; OUTFOX_SEARCH_USER_ID=-42824455@10.169.0.84; JSESSIONID=aaa-Nwxc8TZrmg1pcQhxw; fanyi-ad-id=49843; fanyi-ad-closed=1; ___rl__test__cookies=1536653052506',
        'Referer': 'http://fanyi.youdao.com/',
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'
    }

    url = 'http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule'
    form = {
        'i': trans_str,
        'from': 'AUTO',
        'to': 'AUTO',
        'smartresult': 'dict',
        'client': 'fanyideskweb',
        'salt': salt,
        'sign': sign,
        'doctype': 'json',
        'version': '2.1',
        'keyfrom': 'fanyi.web',
        'action': 'FY_BY_CLICKBUTTION',
        'typoResult': 'false',
    }

    response = requests.post(url, data=form, headers=headers)

    res_dict = response.json()
    translated_str = res_dict['translateResult'][0][0]['tgt']

    return translated_str


if __name__ == '__main__':
    while True:
        trans_str = input('请输入比较有意思的词语：')
        print(translate(trans_str))
```

---

###  雪球网的爬取
```python

import requests
import json
import mysqlhelper
headers = {
    'Accept':'*/*',
    # 'Accept-Encoding':'gzip, deflate, sdch, br',
    'Accept-Language':'zh-CN,zh;q=0.8',
    'Connection':'keep-alive',
    'Cookie':'aliyungf_tc=AQAAAKPgPEfvRgEAUhVFeQbSlA5+jFfh; xq_a_token=9c75d6bfbd0112c72b385fd95305e36563da53fb; xq_a_token.sig=-6-bcHntQlhRjsyrvsY2IGwh-B4; xq_r_token=9ad364aac7522791166c59720025e1f8f11bf9eb; xq_r_token.sig=usx1_hrblByw-9h0cXk1yLIUlL4; u=571536671880142; device_id=5b1260d03e2998f709ec489848aed7ba; _gat_gtag_UA_16079156_4=1; Hm_lvt_1db88642e346389874251b5a1eded6e3=1536671879,1536672134,1536672138; Hm_lpvt_1db88642e346389874251b5a1eded6e3=1536672138; _ga=GA1.2.100884861.1536671879; _gid=GA1.2.1528847784.1536671879',
    'Host':'xueqiu.com',
    'Referer':'https://xueqiu.com/',
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
    'X-Requested-With':'XMLHttpRequest',
}
helper =mysqlhelper.MysqlHelper()
num = 20307736
for i in range(10):
    num -= 15
    url = 'https://xueqiu.com/v4/statuses/public_timeline_by_category.json?since_id=-1&max_id=%s&count=15&category=-1'%(num)
    response = requests.get(url,headers=headers)
    # if i < 1:
    #     print(response.text)

    html_dict = response.json()
    for title_list in html_dict['list']:
        data = json.loads(title_list['data'])
        title = data['title']
        description = data['description']

        # 我们插入的时候的insert语句
        insert_sql = 'insert into xueqiu(title,description) values(%s, %s)'
        # 需要用到的数据的内容
        data = (title, description)
        # 执行这个insert语句，将数据存储到mysql中
        helper.execute_modify_sql(insert_sql, data)

        # print(description)
        # print('----------')
```

### 知乎网的爬取

```python
import requests

headers={
    'authority':  'www.zhihu.com',
    'method':  'GET',
    # ':path':  '/api/v4/search_v3?t=general&q=%E7%88%AC%E8%99%AB&correction=1&offset=15&limit=10&search_hash_id=dc4fe7508a4a19cccb522f4647dcaceb',
    'scheme':  'https',
    'accept':  '*/*',
    # 'accept-encoding':  'gzip, deflate, sdch, br',
    'accept-language':  'zh-CN,zh;q=0.8',
    'cookie':'tgw_l7_route=56f3b730f2eb8b75242a8095a22206f8; _zap=44e2e7dd-3191-49d0-8844-80193edea7f9; _xsrf=mYN0PN27KYiyMsjPt3nOx4hgihSg9voA; d_c0="ANBmZgZQMg6PTidZdVKx4vj_bYDFo-kyhfo=|1536674589"; capsion_ticket="2|1:0|10:1536674791|14:capsion_ticket|44:ZjFhYjdlNWJkZGJmNDZiYzkyOWVkOTI5YzExYjg4ODU=|a474ebb5c5415c15e25569963bada7ac6cec2363df13d5fc6e1f3d8ac075615b"; z_c0="2|1:0|10:1536674828|4:z_c0|92:Mi4xanhWY0NRQUFBQUFBMEdabUJsQXlEaVlBQUFCZ0FsVk5EQnFGWEFEUXh1eUFDTnJMbXhBeXlPSXZKMWVhYnF5d2VR|b4c5915e3bed3529fb47b0991083bd3d2f47f519c066dea70a503f29d738f724"; unlock_ticket="AIDg-QdCkw0mAAAAYAJVTRTTl1tSK7R4Zdor7G3cvPZRv-EPW_4ZXg=="; q_c1=b7e0ede5fa344f43a3c8027a2c78fadb|1536674828000|1536674828000',
    'referer':  'https://www.zhihu.com/search?type=content&q=%E7%88%AC%E8%99%AB',
    'user-agent':  'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
}
num = 5
for i in range(20):
    num += 10
    url = 'https://www.zhihu.com/api/v4/search_v3?t=general&q=%E7%88%AC%E8%99%AB&correction=1&offset='+str(num)+'&limit=10&search_hash_id=dc4fe7508a4a19cccb522f4647dcaceb'
    response = requests.get(url,headers=headers)
    html_dict = response.json()
    for v in html_dict['data']:
        if 'highlight' in v:
            description =v['highlight']['description']
            print(description)
```





